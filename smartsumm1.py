# -*- coding: utf-8 -*-
"""SmartSumm1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hi0B5Zd9L_p2O80AkPPK9EBP-qf7lHPh
"""

!pip install sumy

!pip install rouge_score

import nltk
nltk.download('punkt')
nltk.download('punkt_tab') # Add this line to download the missing resource

import pandas as pd
from tqdm import tqdm
from transformers import BartTokenizer, BartForConditionalGeneration, T5Tokenizer, T5ForConditionalGeneration
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.text_rank import TextRankSummarizer
from rouge_score import rouge_scorer
import matplotlib.pyplot as plt

# قراءة البيانات
df = pd.read_csv("xsum.csv")
subset = df.head(50)

# ========== BART ==========
bart_tokenizer = BartTokenizer.from_pretrained("facebook/bart-large-xsum")
bart_model = BartForConditionalGeneration.from_pretrained("facebook/bart-large-xsum")
bart_summaries = []
for text in tqdm(subset["input"].tolist(), desc="BART Summaries"):
    inputs = bart_tokenizer.encode(text, return_tensors="pt", max_length=1024, truncation=True)
    summary_ids = bart_model.generate(inputs, max_length=60, min_length=10, length_penalty=2.0, num_beams=4, early_stopping=True)
    summary = bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    bart_summaries.append(summary)
subset["bart_summary"] = bart_summaries

# ========== T5 ==========
t5_tokenizer = T5Tokenizer.from_pretrained("t5-base")
t5_model = T5ForConditionalGeneration.from_pretrained("t5-base")
t5_summaries = []
for text in tqdm(subset["input"].tolist(), desc="T5 Summaries"):
    input_text = "summarize: " + text
    inputs = t5_tokenizer.encode(input_text, return_tensors="pt", max_length=512, truncation=True)
    summary_ids = t5_model.generate(inputs, max_length=60, min_length=10, length_penalty=2.0, num_beams=4, early_stopping=True)
    summary = t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    t5_summaries.append(summary)
subset["t5_summary"] = t5_summaries

# ========== TextRank ==========
textrank_summaries = []
for text in tqdm(subset["input"].tolist(), desc="TextRank Summaries"):
    parser = PlaintextParser.from_string(text, Tokenizer("english"))
    summarizer = TextRankSummarizer()
    summary = summarizer(parser.document, 3)
    joined = " ".join(str(sentence) for sentence in summary)
    textrank_summaries.append(joined)
subset["textrank_summary"] = textrank_summaries

# ========== FLAN-T5 + Prompt Engineering ==========
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

flan_tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-base")
flan_model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base")
flan_summaries = []

for text in tqdm(subset["input"].tolist(), desc="FLAN-T5 Summaries"):
    input_text = "Summarize the following in a concise and informative way:\n" + text.strip()
    inputs = flan_tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True)
    summary_ids = flan_model.generate(inputs["input_ids"], max_length=60, min_length=10, length_penalty=2.0, num_beams=4, early_stopping=True)
    summary = flan_tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    flan_summaries.append(summary)

subset["flan_summary"] = flan_summaries

# ========== ROUGE ==========
scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

def compute_rouge(refs, gens):
    r1, r2, rL = [], [], []
    for ref, gen in zip(refs, gens):
        scores = scorer.score(ref, gen)
        r1.append(scores["rouge1"].fmeasure)
        r2.append(scores["rouge2"].fmeasure)
        rL.append(scores["rougeL"].fmeasure)
    return r1, r2, rL

bart_r1, bart_r2, bart_rL = compute_rouge(subset["target"], subset["bart_summary"])
t5_r1, t5_r2, t5_rL = compute_rouge(subset["target"], subset["t5_summary"])
txt_r1, txt_r2, txt_rL = compute_rouge(subset["target"], subset["textrank_summary"])
flan_r1, flan_r2, flan_rL = compute_rouge(subset["target"], subset["flan_summary"])

def print_rouge(name, r1, r2, rL):
    print(f"\n{name} ROUGE Scores")
    print(f"ROUGE-1: {sum(r1)/len(r1):.4f}")
    print(f"ROUGE-2: {sum(r2)/len(r2):.4f}")
    print(f"ROUGE-L: {sum(rL)/len(rL):.4f}")

print_rouge("BART", bart_r1, bart_r2, bart_rL)
print_rouge("T5", t5_r1, t5_r2, t5_rL)
print_rouge("TextRank", txt_r1, txt_r2, txt_rL)
print_rouge("FLAN-T5", flan_r1, flan_r2, flan_rL)

# ========== Visualization ==========
labels = ["ROUGE-1", "ROUGE-2", "ROUGE-L"]
x = range(len(labels))

plt.bar([i - 0.45 for i in x], [sum(bart_r1)/len(bart_r1), sum(bart_r2)/len(bart_r2), sum(bart_rL)/len(bart_rL)], width=0.2, label="BART")
plt.bar([i - 0.15 for i in x], [sum(t5_r1)/len(t5_r1), sum(t5_r2)/len(t5_r2), sum(t5_rL)/len(t5_rL)], width=0.2, label="T5")
plt.bar([i + 0.15 for i in x], [sum(txt_r1)/len(txt_r1), sum(txt_r2)/len(txt_r2), sum(txt_rL)/len(txt_rL)], width=0.2, label="TextRank")
plt.bar([i + 0.45 for i in x], [sum(flan_r1)/len(flan_r1), sum(flan_r2)/len(flan_r2), sum(flan_rL)/len(flan_rL)], width=0.2, label="FLAN-T5")

plt.xticks(x, labels)
plt.ylabel("ROUGE F1 Score")
plt.title("Summarization Comparison (50 Samples)")
plt.legend()
plt.show()

# ========== Save Results ==========
subset.to_csv("xsum_all_models_comparison.csv", index=False)
print("Saved to xsum_all_models_comparison.csv")

# ========== Show Sample Outputs ==========
for i in range(2):
    print("\n--- Example", i+1, "---")
    print("Original:", subset['input'].iloc[i])
    print("Target:", subset['target'].iloc[i])
    print("BART:", subset['bart_summary'].iloc[i])
    print("T5:", subset['t5_summary'].iloc[i])
    print("TextRank:", subset['textrank_summary'].iloc[i])
    print("FLAN-T5:", subset['flan_summary'].iloc[i])